{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "# CONFIGURING GENAI KEY\n",
    "api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = genai.list_models()\n",
    "\n",
    "for model in models:\n",
    "    print(model.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "# Function to extract text from a PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data = extract_text_from_pdf(r\"C:\\Users\\ASUS\\Documents\\GitHub\\Physio-Chatbot\\Sources\\Tidy's Physiotherapy.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "# Function to split the extracted text into chunks\n",
    "def split_text_into_chunks(text):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted_chunks=split_text_into_chunks(extracted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, chunk in enumerate(splitted_chunks[:5]):\n",
    "    print(f\"Chunk {i}: {chunk[:100]}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_query_embedding(query):\n",
    "    response = genai.embed_content(\n",
    "        model=\"models/text-embedding-004\",  # Replace with a valid embedding model\n",
    "        content=query\n",
    "    )\n",
    "\n",
    "    if isinstance(response, dict) and 'embedding' in response:\n",
    "        return response['embedding']\n",
    "    else:\n",
    "        raise ValueError(f\"Failed to generate embeddings for query: {query}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate embeddings for the text chunks\n",
    "def generate_gemini_embeddings(chunks):\n",
    "    \"\"\"\n",
    "    Generates embeddings for given text chunks using an appropriate Gemini embedding model.\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    for chunk in chunks:\n",
    "        if chunk.strip():  # Ensure the chunk is not empty\n",
    "            response = genai.embed_content(\n",
    "                model=\"models/text-embedding-004\", \n",
    "                content=chunk[:500]\n",
    "            )\n",
    "            \n",
    "            # Directly extract the embedding\n",
    "            if isinstance(response, dict) and 'embedding' in response:\n",
    "                embeddings.append(response['embedding'])\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_chunks=generate_gemini_embeddings(splitted_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "papi_key = os.getenv(\"PINECONE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "# Initialize Pinecone\n",
    "pc = Pinecone(api_key=papi_key)\n",
    "\n",
    "index = pc.Index(\"physio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsert_embeddings_in_batches(text_chunks, embeddings, batch_size=100):\n",
    "    vectors = []\n",
    "    \n",
    "    for i, embedding in enumerate(embeddings):\n",
    "        # Create metadata for each chunk\n",
    "        metadata = {\"text\": text_chunks[i], \"source\": \"your_document_source\"}\n",
    "        vector = {\n",
    "            \"id\": f\"vec{i}\",  # Unique ID for each vector\n",
    "            \"values\": embedding,  # The embedding values\n",
    "            \"metadata\": metadata  # Metadata for the chunk\n",
    "        }\n",
    "        vectors.append(vector)\n",
    "        \n",
    "        # Batch upsert every `batch_size` chunks\n",
    "        if (i + 1) % batch_size == 0 or (i + 1) == len(embeddings):\n",
    "            index.upsert(vectors=vectors, namespace=\"ns1\")\n",
    "            vectors = []  # Clear the list after each batch\n",
    "\n",
    "# Call the function to upsert embeddings in batches\n",
    "upsert_embeddings_in_batches(splitted_chunks, embedded_chunks, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_query_embedding(query):\n",
    "    response = genai.embed_content(\n",
    "        model=\"models/text-embedding-004\",  # Replace with a valid embedding model\n",
    "        content=query\n",
    "    )\n",
    "\n",
    "    if isinstance(response, dict) and 'embedding' in response:\n",
    "        return response['embedding']\n",
    "    else:\n",
    "        raise ValueError(f\"Failed to generate embeddings for query: {query}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_chunks(query_embedding):\n",
    "    query_response = index.query(\n",
    "        vector=query_embedding,\n",
    "        top_k=10,\n",
    "        include_metadata=True,\n",
    "        namespace=\"ns1\"\n",
    "    )\n",
    "    \n",
    "    if \"matches\" not in query_response or not query_response[\"matches\"]:\n",
    "        return [\"No relevant information found. Try rephrasing your query.\"]\n",
    "    \n",
    "    retrieved_chunks = [match['metadata']['text'] for match in query_response['matches']]\n",
    "    return retrieved_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_and_retrieve(query):\n",
    "    # Step 1: Generate the embedding for the query\n",
    "    query_embedding = generate_query_embedding(query)\n",
    "    \n",
    "    # Step 2: Retrieve relevant chunks using the generated embedding\n",
    "    retrieved_texts = retrieve_relevant_chunks(query_embedding)\n",
    "    \n",
    "    # Step 3: Return the retrieved texts\n",
    "    return retrieved_texts\n",
    "\n",
    "# Example Usage\n",
    "query = \"What are the symptoms of anxiety?\"\n",
    "retrieved_texts = query_and_retrieve(query)\n",
    "\n",
    "print(retrieved_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to use Gemini Pro LLM to generate the final answer\n",
    "def generate_answer_with_gemini(query, retrieved_chunks):\n",
    "    # Combine the retrieved chunks into a single context\n",
    "    context = \"\\n\".join(retrieved_chunks)\n",
    "\n",
    "    # Crafting the prompt for PhysioBot\n",
    "    prompt = f\"\"\"\n",
    "    You are a PhysioBOT and you have been asked to provide information from the sources you have read : {context}\n",
    "    A user asks you: \"{query}\"\n",
    "    you must answer in a very friendly and informative way and you must provide the answer in detail and in a way that is easy to understand.\n",
    "    you must give detailed information about the query and provide a detailed answer and also keep in mind that you are not an expert physiotherapist but a chatbot. so end the answer by saying that the user should consult a physiotherapist for more information.\n",
    "    Based on your knowledge, you provide the following answer:\n",
    "    \"\"\"\n",
    "\n",
    "    model = genai.GenerativeModel('models/gemini-1.5-pro-latest')\n",
    "    response = model.generate_content([prompt])\n",
    "    return response.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to handle user input and process the query through GODAI\n",
    "def main():\n",
    "    # Prompt user for input\n",
    "    query = input(\"What would you like to ask PhysioBOT? \")\n",
    "\n",
    "    # Step 1: Embed the query using Gemini Pro\n",
    "    query_embedding = generate_query_embedding(query)\n",
    "\n",
    "    # Step 2: Retrieve relevant chunks from Pinecone based on query embedding\n",
    "    retrieved_chunks = retrieve_relevant_chunks(query_embedding)\n",
    "\n",
    "    # Step 3: Generate an answer using Gemini Pro LLM with the retrieved chunks\n",
    "    answer = generate_answer_with_gemini(query, retrieved_chunks)\n",
    "\n",
    "    # Display the final answer to the user\n",
    "    print(\"\\nPhysioBOTS's Answer:\")\n",
    "    print(answer)\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "physioenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
